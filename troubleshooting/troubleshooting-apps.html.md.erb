---
title: Troubleshooting Applications
---

##<a id='monitor-nats'></a>Monitoring NATS Message Bus Traffic ##

To help troubleshoot an issue with an application, you can monitor NATS message bus traffic related to the application.
To do this:

1. Run `cf target` to target the space containing the application.

1. Run `CF_TRACE=true cf app APPLICATION-NAME | grep guid` to show the GUID of the application.

    <pre class="terminal">
    $ CF_TRACE=true cf app my-new-app | grep guid

    "guid": "0a709d5b-0d9b-400b-900a-81594583b634",
    </pre>

1. Run `bosh ssh` to open a secure shell into a NATS node.

1. Run `nats-sub -s "nats://${NATS_USERNAME}:${NATS_PASSWORD}@${NATS_HOST}:${NATS_PORT}" ">" | grep APPLICATION-GUID` to subscribe to NATS and show messages related to the application.

##<a id='htop'></a>Tracking Down an Application by CPU Usage ##

Applications do not always functions as designed.
If you notice a sharp decrease in available staging VMs or an unanticipated
increase in CPU, disk utilization, or memory utilization across the VMs in your
deployment, use the following steps to determine if a specific application is at
fault.

1. Run `bosh ssh` to open a secure shell into the DEA node that is experiencing the unexpected high resource usage.

1. Install the [htop](http://hisham.hm/htop/) interactive process viewer.
In a terminal window, run `htop` to launch the application.

    <%= image_tag("../images/htop/htop-open.png") %>

1. Press the `F6` key and use the arrow keys to sort the htop output by the "TIME+" column.
Processes using excessive resources display a TIME+ value significantly greater than other processes.
Note the names of each of these processes.

    <%= image_tag("../images/htop/htop-name.png") %>

1. Press the `F5` key to view all the processes as a tree.


1. Press the `/` key to search by the name of the processes. Locate the names of any processes found to be using excessive resources. Note the `wshd` command above each of these processes. Each of these `wshd` commands is a Warden handle for an application.

    In this example, the `wshd` command is `17vg99ooo1n`:
    <%= image_tag("../images/htop/htop-find.png") %>

1. Press the `F10` key to exit `htop`.

1.  Run `sudo su -` to enter the root environment with root privileges.

1. Search `/var/vcap/data/dea_next/db/instances.json` to find the Warden handles
identified using `htop`.
The JSON object containing a Warden handle also contains the application GUID
and name.

    In this example, the JSON object containing the Warden handle `17vg99ooo1n` also contains the application GUID `a8a5fb5e-f41d-49b5-af18-576a28f80e7f` and name `console-workers`.

    <%= image_tag("../images/htop/htop-log-handle.png") %>

    <%= image_tag("../images/htop/htop-log-name.png") %>

##<a id='hm9000'></a>Viewing the HM9000 Data Store ##

The HM9000 data store contains information about every VM and application in a
deployment.
Reviewing this data store can provide insights into the state of a Cloud Foundry
deployment as an aid to troubleshooting.

Use the following processes to view the contents of the HM9000 data store.

1. Run `bosh ssh hm9000_z1/0` to open a secure shell into the HM9000 VM.
1. Run `/var/vcap/packages/hm9000/hm9000 dump --config=/var/vcap/jobs/hm9000/config.json > /tmp/hm9000ds`.
This command outputs the data store into a text file, `/tmp/hm9000ds`.

### Retrieve an example backend (internal DEA network host:port) for a particular URL

Send a X-Vcap-Trace header on your request to an app with the appropriate secret
key, and the router will add a X-Vcap-Backend response header identifying the
DEA that was used to service the request.

The value of the X-Vcap-Trace needs to correspond to the router configuration
"trace\_key" as can be seen in the [router config test](https://github.com/cloudfoundry/gorouter/blob/58f54267c43eb52e01b531ee51281f7d48408f3e/src/router/config/config_test.go#L101). If you are using BOSH, that
value is set using "properties.router.trace\_key".

For example, with curl this would be:

```
$ curl -H 'X-Vcap-Trace: 222' http://app.example.com -v
* About to connect() to app.example.com port 80 (#0)
*   Trying 10.242.15.12...
* connected
* Connected to app.example.com (10.242.15.12) port 80 (#0)
> GET / HTTP/1.1
> User-Agent: curl/7.24.0 (x86_64-apple-darwin12.0) libcurl/7.24.0 OpenSSL/0.9.8r zlib/1.2.5
> Host: app.example.com
> Accept: */*
> X-Vcap-Trace: 22
>
< HTTP/1.1 200 OK
< Content-Length: 40
< Content-Type: text/html;charset=utf-8
< Date: Sat, 16 Mar 2013 14:17:21 GMT
< Server: WEBrick/1.3.1 (Ruby/1.9.2/2012-04-20)
< X-Content-Type-Options: nosniff
< X-Frame-Options: SAMEORIGIN
< X-Vcap-Backend: 10.242.15.233:61003
< X-Vcap-Router: 10.242.15.86
< X-Xss-Protection: 1; mode=block
```
The response shows the IP of the router in the X-Vcap-Router header and the
IP:HOST of the backend DEA that responded to this request in the X-Vcap-Backend
header.

### pre-req BOSH setup

* Target a deployment
* bosh target ...

### instructions for finding out the host:port list for a running app:

* bosh vms and find a router ip
* ssh ip
* curl 127.0.0.1:23456 and you should get the routing table returned, the port could vary by configuration, details in https://github.com/cloudfoundry/gorouter/commit/b0aad822d2b6c005e249e391e4e0a38497c3e23c

### going to the warden container of an app once you have the ip:port

* ssh ip, (need a tool to parse /var/vcap/data/dea_next/db/instances.json on port or name)
    * the warden handle in the same json hash as the name/port
    * construct a bash wsh cli and exec
        * you're in the warden
